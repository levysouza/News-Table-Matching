{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import gensim as gs\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import tensorflow_hub as hub\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from heapq import nsmallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_articles = pd.read_csv('../dataset/test_articles_ourdata.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_index = pd.read_csv('../dataset/fixed_test_set_ourdata.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(ID_goal,ranked_tables_ID):\n",
    "    \n",
    "    accuracy = 0\n",
    "    \n",
    "    for table_ID in ranked_tables_ID:\n",
    "        \n",
    "        if table_ID[0] == ID_goal:\n",
    "    \n",
    "            accuracy = 1\n",
    "            break;\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mrr(ID_goal,ranked_tables_ID):\n",
    "    \n",
    "    accuracy = 0\n",
    "    index_match = 1\n",
    "\n",
    "    for idTable in ranked_tables_ID:\n",
    "        \n",
    "        if idTable[0] == ID_goal:\n",
    "    \n",
    "            accuracy = 1/index_match\n",
    "            break;\n",
    "        \n",
    "        index_match = index_match + 1\n",
    "   \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_articles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_index.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_topk = [1,5,10,20,50]\n",
    "\n",
    "result = []\n",
    "\n",
    "#evaluating each topk value\n",
    "for TOP_K in evaluate_topk:\n",
    "\n",
    "    accuracy = []\n",
    "    mrr = []\n",
    "\n",
    "    for i, row in tqdm(test_articles.iterrows()):\n",
    "\n",
    "        #current article values\n",
    "        article_ID = row['article_id']\n",
    "        article_title_text = str(row['article_title'])\n",
    "        article_meta_description_text = str(row['article_meta_description'])\n",
    "        article_keywords_text = str(row['article_keywords'])\n",
    "\n",
    "        #embedding and model variables\n",
    "        article_title = []\n",
    "        title_table = []\n",
    "        ranked_tables_model = []\n",
    "\n",
    "        #return index\n",
    "        return_index = fixed_index.loc[fixed_index['label_index'] == row['article_id']]\n",
    "        \n",
    "        article_title.append(article_title_text+\" \"+article_meta_description_text+\" \"+article_keywords_text)\n",
    "        embedding_articles = embed(article_title)\n",
    "        article_dense_vector = []\n",
    "        for current_embedding in embedding_articles:\n",
    "            article_dense_vector.append(current_embedding.numpy())\n",
    "\n",
    "        #creating embedding \n",
    "        for i, row in return_index.iterrows():\n",
    "            \n",
    "            title_table.append(str(row['table_page_title'])+\" \"+str(row['table_page_main_passage'])+\" \"+str(row['table_page_keywords']))\n",
    "\n",
    "        embedding_tables = embed(title_table)\n",
    "        tables_dense_vector = []\n",
    "        for current_embedding in embedding_tables:\n",
    "            tables_dense_vector.append(current_embedding.numpy())\n",
    "        \n",
    "        distance_vector = pairwise_distances(article_dense_vector, tables_dense_vector, metric='cosine')\n",
    "        \n",
    "        #creating the final dataframe\n",
    "        for i in range(0,len(distance_vector[0])):\n",
    "\n",
    "            ranked_tables_model.append([return_index.iloc[i]['table_page_id'],return_index.iloc[i]['table_page_title'],distance_vector[0][i]]) \n",
    "\n",
    "        data_frame = pd.DataFrame(ranked_tables_model, columns = ['table_ID', 'table_title','table_ranking']) \n",
    "        data_frame_sorting = data_frame.sort_values('table_ranking')\n",
    "        \n",
    "        selected_top = data_frame_sorting.head(TOP_K)\n",
    "#         max_score = selected_top['table_ranking'].max()\n",
    "#         draw_tables_socres = data_frame_sorting[data_frame_sorting['table_ranking'] <= max_score]\n",
    "        final_ranked_tables = selected_top.iloc[:,0:1].values\n",
    "\n",
    "        #getting topk accuracy\n",
    "        accuracy.append(get_accuracy(article_ID, final_ranked_tables))\n",
    "\n",
    "        #testing mean reciprocal rank at k = 50\n",
    "        if TOP_K == 50:\n",
    "\n",
    "            mrr.append(get_mrr(article_ID, final_ranked_tables))\n",
    "\n",
    "    result.append([\"Acc@\"+str(TOP_K),str(round(np.mean(accuracy),4))])\n",
    "\n",
    "print(\"\")\n",
    "print(result[0])\n",
    "print(result[1])\n",
    "print(result[2])\n",
    "print(result[3])\n",
    "print(result[4])\n",
    "print(\"MRR: \"+str(round(np.mean(mrr),4)) )\n",
    "print(mrr)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
