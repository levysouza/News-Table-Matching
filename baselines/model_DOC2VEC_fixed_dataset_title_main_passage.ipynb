{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import gensim as gs\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import tensorflow_hub as hub\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from heapq import nsmallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_articles = pd.read_csv('../dataset/test_articles_newyork.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_index = pd.read_csv('../dataset/fixed_test_set_newyork_max_recall5.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = Doc2Vec.load('../pre_trained_models/doc2vec.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(ID_goal,ranked_tables_ID):\n",
    "    \n",
    "    accuracy = 0\n",
    "    \n",
    "    for table_ID in ranked_tables_ID:\n",
    "        \n",
    "        if table_ID[0] == ID_goal:\n",
    "    \n",
    "            accuracy = 1\n",
    "            break;\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mrr(ID_goal,ranked_tables_ID):\n",
    "    \n",
    "    accuracy = 0\n",
    "    index_match = 1\n",
    "\n",
    "    for idTable in ranked_tables_ID:\n",
    "        \n",
    "        if idTable[0] == ID_goal:\n",
    "    \n",
    "            accuracy = 1/index_match\n",
    "            break;\n",
    "        \n",
    "        index_match = index_match + 1\n",
    "   \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_articles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_index.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_topk = [1,5,10,20,50]\n",
    "\n",
    "result = []\n",
    "\n",
    "#evaluating each topk value\n",
    "for TOP_K in evaluate_topk:\n",
    "\n",
    "    accuracy = []\n",
    "    mrr = []\n",
    "\n",
    "    for i, row in tqdm(test_articles.iterrows()):\n",
    "\n",
    "        #current article values\n",
    "        article_ID = row['article_key_match']\n",
    "        article_title_text = str(row['article_title'])\n",
    "        article_meta_description_text = str(row['article_meta_description'])\n",
    "\n",
    "        #embedding and model variables\n",
    "        article_title = []\n",
    "        title_table = []\n",
    "        ranked_tables_model = []\n",
    "\n",
    "        #return index\n",
    "        return_index = fixed_index.loc[fixed_index['label_index'] == row['article_key_match']]\n",
    "        \n",
    "        vector_words = tknzr.tokenize(article_title_text+\" \"+article_meta_description_text)\n",
    "        word_embedding = embedding_model.infer_vector(vector_words)\n",
    "        article_title.append(word_embedding)\n",
    "\n",
    "        #creating embedding \n",
    "        for i, row in return_index.iterrows():\n",
    "            \n",
    "            vector_words = tknzr.tokenize(str(row['table_page_title'])+\" \"+str(row['table_page_main_passage']))\n",
    "            word_embedding = embedding_model.infer_vector(vector_words)\n",
    "            title_table.append(word_embedding)\n",
    "        \n",
    "        distance_vector = pairwise_distances(article_title, title_table, metric='cosine')\n",
    "        \n",
    "        #creating the final dataframe\n",
    "        for i in range(0,len(distance_vector[0])):\n",
    "\n",
    "            ranked_tables_model.append([return_index.iloc[i]['table_page_id'],return_index.iloc[i]['table_page_title'],distance_vector[0][i]]) \n",
    "\n",
    "        data_frame = pd.DataFrame(ranked_tables_model, columns = ['table_ID', 'table_title','table_ranking']) \n",
    "        data_frame_sorting = data_frame.sort_values('table_ranking')\n",
    "        \n",
    "        selected_top = data_frame_sorting.head(TOP_K)\n",
    "#         max_score = selected_top['table_ranking'].max()\n",
    "#         draw_tables_socres = data_frame_sorting[data_frame_sorting['table_ranking'] <= max_score]\n",
    "        final_ranked_tables = selected_top.iloc[:,0:1].values\n",
    "\n",
    "        #getting topk accuracy\n",
    "        accuracy.append(get_accuracy(article_ID, final_ranked_tables))\n",
    "\n",
    "        #testing mean reciprocal rank at k = 50\n",
    "        if TOP_K == 50:\n",
    "\n",
    "            mrr.append(get_mrr(article_ID, final_ranked_tables))\n",
    "\n",
    "    result.append([\"Acc@\"+str(TOP_K),str(round(np.mean(accuracy),4))])\n",
    "\n",
    "print(\"\")\n",
    "print(result[0])\n",
    "print(result[1])\n",
    "print(result[2])\n",
    "print(result[3])\n",
    "print(result[4])\n",
    "print(\"MRR: \"+str(round(np.mean(mrr),4)) )\n",
    "print(mrr)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
